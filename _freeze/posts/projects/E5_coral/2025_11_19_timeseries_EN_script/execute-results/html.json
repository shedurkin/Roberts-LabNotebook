{
  "hash": "8642671f4f6b294ce4d115f5236d72de",
  "result": {
    "markdown": "---\ntitle: \"Timeseries: Running noise through the Elastic Net\"\nauthor: \"Kathleen Durkin\"\ndate: \"2025-11-19\"\ncategories: [\"E5-coral\"]\nformat:\n  html:\n    toc: true\nexecute: \n  eval: TRUE\nengine: knitr\nbibliography: ../../../references.bib\n---\n\n\nFirst, made existing Elastic Net code (the timeseries M-Multispecies/26.2 code, which includes scaling and WGBS conversion to M-values) into a script that can be run from command line with various input files, parameters, etc.\\\n\\\nRunning as normal (from a conda environment):\n\n\n::: {.cell}\n\n```{.bash .cell-code}\n(r_enet_rscript) shedurkin@raven:~/timeseries_molecular/M-multi-species/scripts$ Rscript 26.2-ElasticNet-scaling.R \\\n\"https://github.com/urol-e5/timeseries_molecular/raw/refs/heads/main/M-multi-species/output/26-rank35-optimization/lambda_gene_0.2/top_genes_per_component/Component_24_apul_count_matrix.csv\" \\\n\"https://github.com/urol-e5/timeseries_molecular/raw/refs/heads/main/M-multi-species/output/10.1-format-miRNA-counts-updated/Apul_miRNA_counts_formatted_updated.txt\"\\\n\"https://gannet.fish.washington.edu/v1_web/owlshell/bu-github/timeseries_molecular/D-Apul/output/31.5-Apul-lncRNA-discovery/lncRNA_counts.clean.filtered.txt\" \\\n\"https://gannet.fish.washington.edu/gitrepos/urol-e5/timeseries_molecular/D-Apul/output/22.5-Apul-multiomic-SR/Apul-filtered-WGBS-CpG-counts.csv\" \\\n\"../../M-multi-species/data/rna_metadata.csv\" \\\n\"../output/26.2-ElasticNet-scaling-test\" \\\n\"ACR-225-TP1\" \\\n0.5 \\\n100 \\\n50 \\\n0.5\n```\n:::\n\n\nThen want to generate a \"fake\" predictor set of just noise. For now I'll choose to (a) use a fully randomized predictor set, instead of \"spiking\" noise into the real data, and (b) to generate the random data empirically\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmake_noise_empirical <- function(X) {\n  X_noise <- apply(X, 2, function(col) {\n    sample(col, length(col), replace = TRUE)\n  })\n  colnames(X_noise) <- colnames(X)\n  rownames(X_noise) <- rownames(X)\n  X_noise\n}\n\nmake_noise_empirical_safe <- function(X) {\n\n  X_noise <- apply(X, 2, function(col) {\n    sample(col, length(col), replace = TRUE)\n  })\n\n  # Ensure row/col names preserved\n  colnames(X_noise) <- colnames(X)\n  rownames(X_noise) <- rownames(X)\n\n  # Fix rows that become all-zero\n  zero_rows <- which(rowSums(X_noise > 0) == 0)\n  if (length(zero_rows) > 0) {\n    for (i in zero_rows) {\n      # Give the row a single small positive value\n      # Put it in a random column\n      j <- sample(seq_len(ncol(X_noise)), 1)\n      X_noise[i, j] <- 1\n    }\n  }\n\n  return(X_noise)\n}\n\nmiRNA_noise <- make_noise_empirical(miRNA)\nlncRNA_noise <- make_noise_empirical(lncRNA)\nWGBS_noise <- make_noise_empirical(WGBS)\n```\n:::\n\n::: {.cell}\n\n```{.bash .cell-code}\n(r_enet_rscript) shedurkin@raven:~/timeseries_molecular/M-multi-species/scripts$ Rscript 26.2-ElasticNet-scaling.R \\\n\"https://github.com/urol-e5/timeseries_molecular/raw/refs/heads/main/M-multi-species/output/26-rank35-optimization/lambda_gene_0.2/top_genes_per_component/Component_24_apul_count_matrix.csv\" \\\n\"../output/26.2-ElasticNet-scaling-test-noise/miRNA_noise_empirical.txt\" \\\n\"../output/26.2-ElasticNet-scaling-test-noise/lncRNA_noise_empirical.txt\" \\\n\"../output/26.2-ElasticNet-scaling-test-noise/WGBS_noise_empirical.csv\" \\\n\"../../M-multi-species/data/rna_metadata.csv\" \\\n\"../output/26.2-ElasticNet-scaling-test-noise\" \\\n\"ACR-225-TP1\" \\\n0.5 \\\n100 \\\n50 \\\n0.5\n```\n:::\n\n\nI did a preliminary run of the EN with A.pulchra randomized input, with just a few replicates, and the results were pretty striking -- *no* genes were \"consistently well predicted\" (mean R\\^2 \\> 0.5 across replicates). In comparison, \\~60 of the 100 response genes were consistently well-predicted by the actual A.pulchra predictor set. This preliminarily suggests to me that the predictors do have significant predictive value, and the model isn't just being driven by the fact there are a lot of inputs which could fortuitously correlate without biological meaning.\n\nI'm going to start longer runs of each (Apul EN with actual predictors, and Apul EN with randomized predictors), using 100 replicates during initial boostrapping and 50 replicates during reduced-set bootstrapping, and see what we get.\n\n## EN using normal predictor set, bootstrap 1\n\n100, bootstrap 2: 50\n\n![](https://github.com/urol-e5/timeseries_molecular/blob/main/M-multi-species/output/26.2-ElasticNet-scaling-test/bootstrap1_performance_errorbars.png?raw=true)\n\n![EN run on only well-predicted genes](https://github.com/urol-e5/timeseries_molecular/blob/main/M-multi-species/output/26.2-ElasticNet-scaling-test/bootstrap2_performance_errorbars.png?raw=true)\n\nThe true predictor set could well-predict the expression of **67 of the 100 genes**, and when those 67 were selected and run through the EN again (50 reps), nearly all (65) were again consistently well-predicted\n\n## EN using empirically-generated noise for predictors\n\nbootstrap 1: 100, bootstrap 2: 50\n\n![](https://github.com/urol-e5/timeseries_molecular/blob/main/M-multi-species/output/26.2-ElasticNet-scaling-test-noise/bootstrap1_performance_errorbars.png?raw=true)\n\nThe noise predictors were able to consistently **predict the expression of 0 genes**\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}