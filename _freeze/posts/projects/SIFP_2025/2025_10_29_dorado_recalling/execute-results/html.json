{
  "hash": "8cb7847faab9da6ad418e30ffe9ef5d8",
  "result": {
    "markdown": "---\ntitle: \"Dorado -- re-basecalling pod5 files\"\nauthor: \"Kathleen Durkin\"\ndate: \"2025-10-29\"\ncategories: [\"SIFP-2025\"]\nformat:\n  html:\n    toc: true\nexecute: \n  eval: FALSE\nengine: knitr\nbibliography: ../../../references.bib\n---\n\n\nI have a bunch of raw Nanopore output pod5 files, which I need to take through basecalling, demultiplexing, trimming, and genome alignment. Note that several of these steps are performed automatically during sequencing, and I have those output files, but there are differences in software version used. To maintain consistency in model used and types of modifications called, I need to re-do it all from the raw pod5s.\n\nLuckily, all of these steps (modified basecalling, demultiplexing, trimming, alignment) can be done with a single command from the Nanopore `Dorado` software! I've done some preliminary trials using some of the Flongle output in the below code documents:\n\n[G1 L4 Flongle](https://github.com/shedurkin/SIFP-nanopore/blob/main/A-Group1/code/05-G1-Library4-Flongle-Dorado-recall.md)\\\n[G4 L1 Flongle](https://github.com/shedurkin/SIFP-nanopore/blob/main/D-Group4/code/02-G4-Library1-Flongle-Dorado-recall.md)\n\nUnfortunately, this is very computationally intensive. Scaling up from the Flongle runs (50-200 Mb output) to MinION runs (2-8 Gb output) will require significantly more computational power. Re-calling the trial Flongle data took several hours using CPUs alone, so the MinION runs would take days.\n\nI started some CPU-based MinION recalling for Group 4 ([Group4 Library1 MinION recalling](https://github.com/shedurkin/SIFP-nanopore/blob/main/D-Group4/code/03-G4-Library1-MinION-Dorado-recall.Rmd), [Group4 Library2 MinION recalling](https://github.com/shedurkin/SIFP-nanopore/blob/main/D-Group4/code/04-G4-Library2-MinION-Dorado-recall.Rmd)), which has lowest output. However, I want to figure out how to use Hyak GPUs for the Dorado basecalling to significantly reduce the time required.\n\nSam's previously done this (but on Mox and with the older Dorado output file format and software, `fast5` and `Guppy` ) so I'll be basing my work off of [his notebook post](https://robertslab.github.io/sams-notebook/posts/2020/2020-09-04-Data-Wrangling---NanoPore-Fast5-Conversion-to-FastQ-of-C.bairdi-6129_403_26-on-Mox-with-GPU-Node/index.html).\n\nAfter a whole day of modifying and testing the SLURM script (it takes 1-2hrs to test each time, since my jobs have to wait in a `ckpt` queue), I finally got it to work! The big problem I had to figure out was how to modify Sam's script to use *containerized* software -- in [Sam's fast5 basecalling script](https://robertslab.github.io/sams-notebook/posts/2020/2020-09-04-Data-Wrangling---NanoPore-Fast5-Conversion-to-FastQ-of-C.bairdi-6129_403_26-on-Mox-with-GPU-Node/index.html), he just had `Guppy` installed directly on the Mox server. On Klone, however, `Dorado` is available in a container (`/gscratch/srlab/containers/srlab-R4.4-bioinformatics-container-3886a1c.sif`). I found the clue of how to do this by searching Sam's notebook repo, in the directory for sbatch scripts, for the word \"container\", and finding [this script](https://github.com/RobertsLab/sams-notebook/blob/802d4f0801ad65903e64734be65d96153ac4c30f/sbatch_scripts/20241118-cvir-bismark-bowtie2-alignments.sh#L17):\n\n```         \n# Execute Roberts Lab bioinformatics container\n# Binds home directory\n# Binds /gscratch directory\n# Directory bindings allow outputs to be written to the hard drive.\napptainer exec \\\n--home \"$PWD\" \\\n--bind /mmfs1/home/ \\\n--bind /gscratch \\\n/gscratch/srlab/sr320/srlab-bioinformatics-container-586bf21.sif \\\n/gscratch/scrubbed/samwhite/gitrepos/ceasmallr/code/02.01-bismark-bowtie2-alignment-SLURM-array.sh\n```\n\nSam used the `--bind` option when executing the container. After looking in to what that is, I learned that you have to \"bind\" your working directory (the one which contains your input data and output folder) to the container so that it knows where they are. Otherwise the container, which is essentially a self-contained computing environment, won't have access to them. Such a simple fix after sooo long debugging ðŸ¥´\n\nAfter adding binding to my container executions, I finally got a working SLURM batch script! For example, [here's the script](https://github.com/shedurkin/SIFP-nanopore/blob/main/D-Group4/code/03.01_G4L1_MinION_Dorado.sh) used to `Dorado` basecall the sequencing data from the Group 4 Library 1 MinION run:\n\n```         \n#!/bin/bash\n## Job Name\n#SBATCH --job-name=G4L1_MinION_Dorado\n## Allocation Definition\n#SBATCH --account=srlab-ckpt\n#SBATCH --partition=ckpt\n## Resources\n## GPU\n#SBATCH --gres=gpu:2080ti:1\n## Nodes\n#SBATCH --nodes=1\n## Walltime (days-hours:minutes:seconds format)\n#SBATCH --time=0-02:00:00\n## Memory per node\n#SBATCH --mem=120G\n##turn on e-mail notification\n#SBATCH --mail-type=ALL\n#SBATCH --mail-user=kdurkin1@uw.edu\n## Specify the working directory for this job\n#SBATCH --chdir=/gscratch/srlab/kdurkin1/SIFP-nanopore/D-Group4/output/03.01-G4-Library1-MinION-Dorado-recall-GPU/\n\n## Script for running ONT Dorado to perform\n## basecalling (i.e. convert raw ONT pod5 to FastQ) of NanaPore data generated\n## Summer 2025, as psrt of K.Durkin SIFP project\n\n## This script utilizes a GPU node. These nodes are only available as part of the checkpoint\n## partition/account. Since we don't own a GPU node, our GPU jobs are lowest priority and\n## can be interrupted at any time if the node owner submits a new job.\n\n###################################################################################\n# These variables need to be set by user\n\nwd=$(pwd)\n\n# Programs array\n# declare -A programs_array\n# programs_array=(\n# [dorado]=\"apptainer exec --nv --bind /gscratch /gscratch/srlab/containers/srlab-R4.4-bioinformatics-container-3886a1c.sif dorado\"\n# )\n\n\n# Establish variables for more readable code\n\n# Input files directory\nraw_pod5_dir=/gscratch/srlab/kdurkin1/SIFP-nanopore/D-Group4/data/03.01-G4-Library1-MinION-Dorado-recall-GPU/\noutput_dir=/gscratch/srlab/kdurkin1/SIFP-nanopore/D-Group4/output/03.01-G4-Library1-MinION-Dorado-recall-GPU/\ngenome_file=/gscratch/srlab/kdurkin1/SIFP-nanopore/data/GCA_965233905.1_jaEunKnig1.1/GCA_965233905.1_jaEunKnig1.1_genomic.fna\n\n# Output directory\nout_dir=${wd}\n\n# CPU threads\nthreads=28\n\n# Sequencing kit used\nkit=\"SQK-NBD114-96\"\n\n# Flow Cell ID\nflow_cell_id=\"FBD08455\"\n\n# GPU devices setting\nGPU_devices=auto\n\n# Set number of FastQ sequences written per file (0 means all in one file)\nrecords_per_fastq=0\n\n###################################################################################\n\n# Exit script if any command fails\nset -e\n\n# Load CUDA GPU module\nmodule load cuda/12.9.1\n\napptainer exec \\\n--nv \\\n--home \"$PWD\" \\\n--bind /mmfs1/home/ \\\n--bind /gscratch \\\n/gscratch/srlab/containers/srlab-R4.4-bioinformatics-container-3886a1c.sif \\\ndorado basecaller \\\nhac \\\n-r ${raw_pod5_dir}/ \\\n--kit-name SQK-NBD114-96 \\\n--trim 'all' \\\n--reference ${genome_file} \\\n--modified-bases 5mCG_5hmCG 6mA \\\n--device ${GPU_devices} \\\n> ${output_dir}/FBD08455_pass_recalled.bam\n\n\n###################################################################################\n\n# Document programs in PATH (primarily for program version ID)\n{\ndate\necho \"\"\necho \"System PATH for $SLURM_JOB_ID\"\necho \"\"\nprintf \"%0.s-\" {1..10}\necho \"${PATH}\" | tr : n\n} >> system_path.log\n\n\n# Capture program options\nfor program in \"${!programs_array[@]}\"\ndo\n    {\n  echo \"Program options for ${program}: \"\n    echo \"\"\n    ${programs_array[$program]} --help\n    echo \"\"\n    echo \"\"\n    echo \"----------------------------------------------\"\n    echo \"\"\n    echo \"\"\n} &>> program_options.log || true\ndone\n```\n\nNote that I specified a fairly small GPU in this script, `gpu:2080ti:1`, because Hyak has a *lot* of these, and the queue time ended up being much shorter. For larger jobs, a more powerful GPU may be desirable.\n\nI set up and ran scripts to `Dorado` basecall to passed `pod5` files of all 5 MinION sequencing runs by late night of 10/29/2025. By the next morning, all but the G1L4 runs have finished:\n\n| Sequencing Run | Dorado Basecalling Job ID | Runtime  | Output Directory                                                                                                  |\n|-------------|-------------|-------------|---------------------------------|\n| G1L4 MinION    | 30587784                  | 06:56:29 | <https://github.com/shedurkin/SIFP-nanopore/tree/main/A-Group1/output/06.01-G1-Library4-MinION-Dorado-recall-GPU> |\n| G2L2 MinION    | 30552393                  | 03:25:48 | <https://github.com/shedurkin/SIFP-nanopore/tree/main/B-Group2/output/04.01-G2-Library2-MinION-Dorado-recall-GPU> |\n| G2L3 MinION    | 30553071                  | 03:01:26 | <https://github.com/shedurkin/SIFP-nanopore/tree/main/B-Group2/output/05.01-G2-Library3-MinION-Dorado-recall-GPU> |\n| G4L1 MinION    | 31837837                  | 01:27:18 | <https://github.com/shedurkin/SIFP-nanopore/tree/main/D-Group4/output/03.01-G4-Library1-MinION-Dorado-recall-GPU> |\n| G4L2 MinION    | 30550488                  | 01:33:34 | <https://github.com/shedurkin/SIFP-nanopore/tree/main/D-Group4/output/04.01-G4-Library2-MinION-Dorado-recall-GPU> |\n\nI'm soooo excited to have this working, because basecalling on CPUs along was taking a *very*long time. For the G4L1 MinION data, I had it running on CPUs for **24 hours** and it generated **\\~120MB** of output (withuot completing the run). On a GPU, the full **\\~900MB** of output was generated in just an **hour and a half**!\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}