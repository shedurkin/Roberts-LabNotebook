{
  "hash": "49300cb94e94656256842e68bc67fe1e",
  "result": {
    "markdown": "---\ntitle: \"Transferring Nanopore Sequencing Output Files\"\nauthor: \"Kathleen Durkin\"\ndate: \"2025-08-28\"\ncategories: [\"SIFP-2025\"]\nformat:\n  html:\n    toc: true\nexecute: \n  eval: FALSE\nengine: knitr\nbibliography: ../../../references.bib\n---\n\n\nI've been having some difficulty with getting my Nanopore sequencing files (which are both large and numerous) from the sequencing computer onto Gannet, where I can access them for bioinformatic work.\n\nI originally stored the files in my Smithsonian Institution (SI) OneDrive account, but it turns out getting files off of OneDrive is a seemingly impossible task. I've tried the following:\n\n-   SI OneDrive -\\> Gannet, via CloudSync. I can't sync Gannet with the SI OneDrive without admin access, which I won't get.\n\n-   UW OneDrive -\\> Gannet, via CloudSync. Same issue as above, unlikely to be granted UW admin access.\n\n-   Rclone. According to Matt Kweskin, there are also SI access issues that SI IT hasn't resolved.\n\n-   SI OneDrive -\\> Hydra (SI computing cluster) -\\> Gannet, via GLOBUS. Issue is I'll lose access to Hydra once I leave in 2 weeks and I really want to figure out a way that works once I'm gone, in case I need to transfer more files.\n\n-   SI OneDrive -\\> download locally -\\> Gannet. The folders kept downloading incompletely (e.g. subfolders/files were getting skipped), and it felt ripe for a failed/incomplete transfer\n\n-   Sequencing computer -\\> UW Google Drive -\\> Gannet, via Cloudsync. Problem is that the UW Google Drive maxes out at 100Gb, and some of my sequencing folders will be larger than this.\n\nFinally I found something that seems to work:\n\n-   *Compress* the folder on the sequencing computer -\\> UW Google Drive -\\> Gannet, via CloudSync -\\> uncompress on Gannet.\n\nSo, I've compressed 3 of the output folders I have on the sequencing computer, generated md5 checksums for the zipped files, and uploaded them to my UW Google Drive. I then used CloudSync to transfer the files (.zip and .md5) to Gannet. I now want to A) check the md5s to confirm successful file transfer, and B) unzip the files, checking for file corruption.\n\nAll of this was done from a terminal ssh'd into Gannet, I'm just copy-pasting code and output here for documentation purposes.\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nkdurkin1@Gannet:/volume2/web/kdurkin1/SIFP_2025$ ls\n# Group1.md5  Group2_Flongle  Group2_MinION.md5  Group2.zip\n# Group1.zip  Group2.md5      Group2_MinION.zip\n\nkdurkin1@Gannet:/volume2/web/kdurkin1/SIFP_2025$ head Group1.md5\n\n# Algorithm       Hash                                                                   Path\n# ---------       ----                                                                   ----\n# MD5             F7E28FE37E3B910C21641010D7B035D5                                       C:\\Users\\Public\\Documents\\LAB...\n\nkdurkin1@Gannet:/volume2/web/kdurkin1/SIFP_2025$ md5sum Group1.zip\n# f7e28fe37e3b910c21641010d7b035d5  Group1.zip\n\n\nkdurkin1@Gannet:/volume2/web/kdurkin1/SIFP_2025$ head Group2.md5\n\n# Algorithm       Hash                                                                   Path\n# ---------       ----                                                                   ----\n# MD5             9C8DAFB717360464B195FC2A499F7910                                       C:\\Users\\Public\\Documents\\LAB...\n\nkdurkin1@Gannet:/volume2/web/kdurkin1/SIFP_2025$ md5sum Group2.zip\n# 9c8dafb717360464b195fc2a499f7910  Group2.zip\n\n\nkdurkin1@Gannet:/volume2/web/kdurkin1/SIFP_2025$ head Group2_MinION.md5\n\n# Algorithm       Hash                                                                   Path\n# ---------       ----                                                                   ----\n# MD5             FADB27D46B25A6CB5E683813E2D59DC4                                       C:\\Users\\Public\\Documents\\LAB...\n\nkdurkin1@Gannet:/volume2/web/kdurkin1/SIFP_2025$ md5sum Group2_MinION.zip\n# fadb27d46b25a6cb5e683813e2d59dc4  Group2_MinION.zip\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Double check Group1 checksums\nidentical(tolower(\"F7E28FE37E3B910C21641010D7B035D5\"), tolower(\"f7e28fe37e3b910c21641010d7b035d5\"))\n\n# Double check Group2 checksums\nidentical(tolower(\"9C8DAFB717360464B195FC2A499F7910\"), tolower(\"9c8dafb717360464b195fc2a499f7910\"))\n\n# Double check Group2_MinION checksums\nidentical(tolower(\"FADB27D46B25A6CB5E683813E2D59DC4\"), tolower(\"fadb27d46b25a6cb5e683813e2d59dc4\"))\n```\n:::\n\n\nSo all 3 files transferred correctly\n\nNow check for corruption during file compression/decompression\n\nGannet doesn't seem to have the `zip`/`unzip` commands, so will have to use `7z`\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nkdurkin1@Gannet:/volume2/web/kdurkin1/SIFP_2025$ 7z t Group1.zip\n\n# 7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n# p7zip Version 16.02 (locale=en_US.utf8,Utf16=on,HugeFiles=on,64 bits,8 CPUs x64)\n# \n# Scanning the drive for archives:\n# 1 file, 2320416210 bytes (2213 MiB)\n# \n# Testing archive: Group1.zip\n# --\n# Path = Group1.zip\n# Type = zip\n# Physical Size = 2320416210\n# \n# Everything is Ok\n# \n# Folders: 42\n# Files: 2950\n# Size:       2398985870\n# Compressed: 2320416210\n\nkdurkin1@Gannet:/volume2/web/kdurkin1/SIFP_2025$ 7z t Group2.zip\n\n# 7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n# p7zip Version 16.02 (locale=en_US.utf8,Utf16=on,HugeFiles=on,64 bits,8 CPUs x64)\n# \n# Scanning the drive for archives:\n# 1 file, 795592065 bytes (759 MiB)\n# \n# Testing archive: Group2.zip\n# --\n# Path = Group2.zip\n# Type = zip\n# Physical Size = 795592065\n# \n# Everything is Ok\n# \n# Folders: 89\n# Files: 2917\n# Size:       836197911\n# Compressed: 795592065\n\nkdurkin1@Gannet:/volume2/web/kdurkin1/SIFP_2025$ 7z t Group2_MinION.zip\n\n# 7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n# p7zip Version 16.02 (locale=en_US.utf8,Utf16=on,HugeFiles=on,64 bits,8 CPUs x64)\n# \n# Scanning the drive for archives:\n# 1 file, 55773141409 bytes (52 GiB)\n# \n# Testing archive: Group2_MinION.zip\n# --\n# Path = Group2_MinION.zip\n# Type = zip\n# Physical Size = 55773141409\n# 64-bit = +\n# \n# Everything is Ok\n# \n# Folders: 215\n# Files: 8274\n# Size:       56411908485\nCompressed: 55773141409\n```\n:::\n\n\nAll 3 zipped files pass checks, so I can decompress them\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nkdurkin1@Gannet:/volume2/web/kdurkin1/SIFP_2025$ 7z x Group1.zip\n\n# 7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n# p7zip Version 16.02 (locale=en_US.utf8,Utf16=on,HugeFiles=on,64 bits,8 CPUs x64)\n# \n# Scanning the drive for archives:\n# 1 file, 2320416210 bytes (2213 MiB)\n# \n# Extracting archive: Group1.zip\n# --\n# Path = Group1.zip\n# Type = zip\n# Physical Size = 2320416210\n# \n# Everything is Ok\n# \n# Folders: 42\n# Files: 2950\n# Size:       2398985870\n# Compressed: 2320416210\n\n\nkdurkin1@Gannet:/volume2/web/kdurkin1/SIFP_2025$ 7z x Group2.zip\n\n# 7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n# p7zip Version 16.02 (locale=en_US.utf8,Utf16=on,HugeFiles=on,64 bits,8 CPUs x64)\n# \n# Scanning the drive for archives:\n# 1 file, 795592065 bytes (759 MiB)\n# \n# Extracting archive: Group2.zip\n# --\n# Path = Group2.zip\n# Type = zip\n# Physical Size = 795592065\n# \n# Everything is Ok\n# \n# Folders: 89\n# Files: 2917\n# Size:       836197911\n# Compressed: 795592065\n\nkdurkin1@Gannet:/volume2/web/kdurkin1/SIFP_2025$ 7z x Group2_MinION.zip\n\n# 7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n# p7zip Version 16.02 (locale=en_US.utf8,Utf16=on,HugeFiles=on,64 bits,8 CPUs x64)\n# \n# Scanning the drive for archives:\n# 1 file, 55773141409 bytes (52 GiB)\n# \n# Extracting archive: Group2_MinION.zip\n# --\n# Path = Group2_MinION.zip\n# Type = zip\n# Physical Size = 55773141409\n# 64-bit = +\n# \n# Everything is Ok\n# \n# Folders: 215\n# Files: 8274\n# Size:       56411908485\n# Compressed: 55773141409\n\n```\n:::\n\n\nThere are still no warnings, but I noticed that Group1 is missing sub-folders.\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nkdurkin1@Gannet:/volume2/web/kdurkin1/SIFP_2025$ ls Group1/Group1/\n#Library2\n```\n:::\n\n\nIt should contain 4 subfolders, each containing the data from a Flongle run with a different library prep (1-4). However, I only see Library2 there. Since md5s matched and the files unzipped with no issues, there must have been a problem with compressing the file originally.\n\nTry again.\n\nOn the sequencing computer, ensured the Group1 folder has the expected subfolders, re-compressed, generated new md5sum, uploaded both to Google Drive, and synced with Gannet. New checks:\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nkdurkin1@Gannet:/volume2/web/kdurkin1/SIFP_2025$ head Group1.md5\n\n# Algorithm       Hash                                                                   Path\n# ---------       ----                                                                   ----\n# MD5             82BD3B02FADD408F3469C6E294DDDF4E                                       C:\\Users\\Public\\Documents\\LAB...\n\n\nkdurkin1@Gannet:/volume2/web/kdurkin1/SIFP_2025$ md5sum Group1.zip\n# 82bd3b02fadd408f3469c6e294dddf4e  Group1.zip\n\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Double check new Group1 checksums\nidentical(tolower(\"82BD3B02FADD408F3469C6E294DDDF4E\"), tolower(\"82bd3b02fadd408f3469c6e294dddf4e\"))\n```\n:::\n\n\nUncompress and check that the subfolders are there\n\n\n::: {.cell}\n\n```{.bash .cell-code}\n\nkdurkin1@Gannet:/volume2/web/kdurkin1/SIFP_2025$ 7z x Group1.zip\n\n# 7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n# p7zip Version 16.02 (locale=en_US.utf8,Utf16=on,HugeFiles=on,64 bits,8 CPUs x64)\n# \n# Scanning the drive for archives:\n# 1 file, 6440567574 bytes (6143 MiB)\n# \n# Extracting archive: Group1.zip\n# --\n# Path = Group1.zip\n# Type = zip\n# Physical Size = 6440567574\n# 64-bit = +\n# \n# Everything is Ok\n# \n# Folders: 125\n# Files: 10005\n# Size:       6700569546\n# Compressed: 6440567574\n\nkdurkin1@Gannet:/volume2/web/kdurkin1/SIFP_2025$ ls Group1/Group1/\n# Library1  Library2  Library3  Library4\n\n```\n:::\n\n\nOnce all 3 files loaded onto gannet and checked, I removed them from Google Drive, since I only have 100GB of storage there.\n\nFinally, need to generate md5 checksums for all the files in the folders I just synced to Gannet (`Group1`, `Group2`, `Group2_MinION` ) to verify integrity in future file transfers (e.g., Gannet -\\> Raven). (I didn't do this on the original sequencing computer because it only had Windows Powershell as a terminal, and I don't know much powershell syntax)\n\nThe bash code used below will iterate through each subfolder and, if the folder contains files, will generate a checksums.md5 containing md5 hashes for all files in that folder.\n\n\n::: {.cell}\n\n```{.bash .cell-code}\n\nkdurkin1@Gannet:/volume2/web/kdurkin1/SIFP_2025/Group1$ find . -type d -exec sh -c '\n  cd \"{}\" || exit\n  # Check if there are any regular files in the directory\n  if find . -maxdepth 1 -type f | grep -q .; then\n    md5sum * > checksums.md5\n  fi\n' \\;\n\n# md5sum: bam_fail: Is a directory\n# md5sum: bam_pass: Is a directory\n# md5sum: fastq_fail: Is a directory\n# md5sum: fastq_pass: Is a directory\n# md5sum: other_reports: Is a directory\n# md5sum: pod5: Is a directory\n# md5sum: bam_fail: Is a directory\n# md5sum: bam_pass: Is a directory\n# md5sum: fastq_fail: Is a directory\n# md5sum: fastq_pass: Is a directory\n# md5sum: other_reports: Is a directory\n# md5sum: pod5: Is a directory\n# md5sum: bam_fail: Is a directory\n# md5sum: bam_pass: Is a directory\n# md5sum: fastq_fail: Is a directory\n# md5sum: fastq_pass: Is a directory\n# md5sum: other_reports: Is a directory\n# md5sum: pod5: Is a directory\n# md5sum: bam_fail: Is a directory\n# md5sum: bam_pass: Is a directory\n# md5sum: fastq_fail: Is a directory\n# md5sum: fastq_pass: Is a directory\n# md5sum: other_reports: Is a directory\n# md5sum: pod5: Is a directory\n# md5sum: pod5_skip: Is a directory\n\nkdurkin1@Gannet:/volume2/web/kdurkin1/SIFP_2025/Group1$ cd ../Group2\n\nkdurkin1@Gannet:/volume2/web/kdurkin1/SIFP_2025/Group2$ find . -type d -exec sh -c '\n  cd \"{}\" || exit\n  # Check if there are any regular files in the directory\n  if find . -maxdepth 1 -type f | grep -q .; then\n    md5sum * > checksums.md5\n  fi\n' \\;\n# md5sum: bam_fail: Is a directory\n# md5sum: bam_pass: Is a directory\n# md5sum: fastq_fail: Is a directory\n# md5sum: fastq_pass: Is a directory\n# md5sum: other_reports: Is a directory\n# md5sum: pod5_fail: Is a directory\n# md5sum: pod5_pass: Is a directory\n# md5sum: bam_fail: Is a directory\n# md5sum: bam_pass: Is a directory\n# md5sum: fastq_fail: Is a directory\n# md5sum: fastq_pass: Is a directory\n# md5sum: other_reports: Is a directory\n# md5sum: pod5_fail: Is a directory\n# md5sum: pod5_pass: Is a directory\n\nkdurkin1@Gannet:/volume2/web/kdurkin1/SIFP_2025/Group2$ cd ../Group2_MinION\n\nkdurkin1@Gannet:/volume2/web/kdurkin1/SIFP_2025/Group2_MinION$ find . -type d -exec sh -c '\n  cd \"{}\" || exit\n  # Check if there are any regular files in the directory\n  if find . -maxdepth 1 -type f | grep -q .; then\n    md5sum * > checksums.md5\n  fi\n' \\;\n# md5sum: bam_fail: Is a directory\n# md5sum: bam_pass: Is a directory\n# md5sum: fastq_fail: Is a directory\n# md5sum: fastq_pass: Is a directory\n# md5sum: other_reports: Is a directory\n# md5sum: pod5_fail: Is a directory\n# md5sum: pod5_pass: Is a directory\n# md5sum: bam_fail: Is a directory\n# md5sum: bam_pass: Is a directory\n# md5sum: fastq_fail: Is a directory\n# md5sum: fastq_pass: Is a directory\n# md5sum: other_reports: Is a directory\n# md5sum: pod5_fail: Is a directory\n# md5sum: pod5_pass: Is a directory\n\n```\n:::\n\n\nWhen I tried viewing my SIFP_2025 folder on Gannet from the public-facing web server, I couldn't see it -- instead I got the error message \"*Forbidden. You don't have permission to access this resource. Server unable to read htaccess file. Denying acces to be safe*\"\n\nThis turned out to be a permissions issue, so I updated permissions. I want others to have:\n\n-   **read access** (e.g., 644 -\\> -rw-r--r--) to **files**, which will permit people to see and download files, and\n\n-   **read and execute access** (e.g., 755 -\\> drwxr-xr-x) to **directories**, which will allow people to see and access the directory's contents.\n\n\n::: {.cell}\n\n```{.bash .cell-code}\n\n# find all subdirectories and set read + execute permissions for others\nkdurkin1@Gannet:/volume2/web/kdurkin1/SIFP_2025/Group1$ find . -type d -exec chmod 755 {} \\;\n\nkdurkin1@Gannet:/volume2/web/kdurkin1/SIFP_2025/Group1$ find . -type f -exec chmod 644 {} \\;\n\n# check\nkdurkin1@Gannet:/volume2/web/kdurkin1/SIFP_2025/Group1$ ls -ld .\n# drwxr-xr-x 3 kdurkin1 users 4096 Aug 29 08:31 .\n\nkdurkin1@Gannet:/volume2/web/kdurkin1/SIFP_2025/Group1$ ls -ld ./Group1/Library1/20250812_1139_MD-101223_AYW935_f9a34344/checksums.md5\n# -rw-r--r-- 1 kdurkin1 users 698 Aug 29 08:36 ./Group1/Library1/20250812_1139_MD-101223_AYW935_f9a34344/checksums.md5\n\n# Repeat for Group2 and Group2_MinION\nkdurkin1@Gannet:/volume2/web/kdurkin1/SIFP_2025/Group1$ cd ../Group2\nkdurkin1@Gannet:/volume2/web/kdurkin1/SIFP_2025/Group2$ find . -type d -exec chmod 755 {} \\;\nkdurkin1@Gannet:/volume2/web/kdurkin1/SIFP_2025/Group2$ find . -type f -exec chmod 644 {} \\;\n\nkdurkin1@Gannet:/volume2/web/kdurkin1/SIFP_2025/Group2$ cd ../Group2_MinION\nkdurkin1@Gannet:/volume2/web/kdurkin1/SIFP_2025/Group2_MinION$ find . -type d -exec chmod 755\n{} \\;\nkdurkin1@Gannet:/volume2/web/kdurkin1/SIFP_2025/Group2_MinION$ find . -type f -exec chmod 644\n{} \\;\n \n```\n:::\n\n\nAfter granting non-owner read and execute permissions, can see files from public-facing server!\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}